{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyG7KgakemZ0"
      },
      "source": [
        "#Discussion Question:\n",
        "Autonomous weapons, such as drones and missiles that use AI or ML, are controversial (Russell and Norvig p. 988-989).  They take the human out of the decision loop for whether to kill a target, which many view as unethical; they are \"scalable weapons of mass destruction\" where a million drones could fit inside a shipping container; and they have the potential to be buggy and run amok.  Reinforcement learning research, contests, and prototypes, such as DeepMind's Starcraft-playing AlphaStar, seem to directly advance this kind of work.  Should researchers avoid reinforcement learning (or perhaps particular RL research) as ethically problematic, or should researchers not worry about it?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGyPQRuGd1d2"
      },
      "source": [
        "1) When is it impossible to do value iteration, but you instead must use some kind of temporal difference learning instead?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzvHI9-TekqT"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oni_p0LGekqU"
      },
      "source": [
        "2) On moving East from a particular square, there is a 0.25 probability that the agent falls in a pit (-100 utility reward) and a 0.75 chance that the agent avoids the pit and gets gold (+100 utility reward).  The resulting state s' either way is currently valued at U(s') = 5.  The agent currently believes the square it's on is worth 10, but then it moves right and gets the gold. Use the temporal difference equation $U(s) = U(s) + \\alpha[R(s,a,s') + \\gamma U(s') - U(s)]$ with a learning rate of 0.1 and discount factor of 0.5 to update the utility of the square the agent just moved from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRyQcuGekqU"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQOZmq8eekqU"
      },
      "source": [
        "3)Suppose the next time the agent makes this move, the agent falls in a pit.  Perform the utility update for the starting square with the result of the previous problem as the starting utility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ6v6bKgekqU"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isdKQGNHekqU"
      },
      "source": [
        "4) An agent moves east from square $s_1$, with a move that currently has a Q-value of 20.  The agent lands in square $s_2$ with no rewards but q-values Q($s_2$, North) = 120, Q($s_2$, East) = 50, Q($s_2$, South) = 50, Q($s_2$, West) = 25.  Update Q($s_1$, East), assuming a learning rate of 0.1 and discount factor of 0.5.  You may find the following equation useful:  $Q(s,a) = Q(s,a) + \\alpha [R(s,a,s') + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ylyc3ZYekqV"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-TWgilKekqV"
      },
      "source": [
        "5) In the previous problem, suppose the actual move of the agent after reaching square $s_2$ is to go East to explore, with a Q-value of 50.  Perform an update to $Q(s_1, East)$ using the SARSA update rule, which updates using the value of the next move actually taken instead of the best action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whH0TXFbekqV"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcFuDZ4cekqV"
      },
      "source": [
        "6) Board game AIs often tend to use utilities of states U(s) without Q-values, while systems that interact with the real world or real-time video games tend to use Q-values Q(a,s).  Why are board games a good fit for temporal difference learning on states?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8nGg2yZekqV"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZeJVA6bkQXo"
      },
      "source": [
        "*The following questions are about the DeepMind Atari games paper, Mnih et al 2013.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KycgMzpitlm"
      },
      "source": [
        "7) What is the policy for exploration that is used by the agent in this work?  Could this have been differently?  If so, what would be the advantages or disadvantages of doing it differently?  If not, why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfAYOVrrkp9K"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqRppSY-lYJ2"
      },
      "source": [
        "8) The preprocessing on the frames included conversion to grayscale and cropping to $84 \\times 84$ from $210 \\times 160$.  Would the paper be more convincing if it used the original images, or does this not matter?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jO8QYZMnetl"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xhumSzoodnM"
      },
      "source": [
        "9) How many hidden layers were there in this work?  In a modern context, is this a lot of hidden layers, or not many?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9ziwkdmolhU"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtcEMYwHpLWM"
      },
      "source": [
        "10) What do you think of their squashing the rewards to be just -1, 1, or 0?  Is this a superficial change, or does it seem important?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ0L57V_pVJv"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHVVpwtaqIws"
      },
      "source": [
        "11) What about changing the frameskipping to be different for Space Invaders?  Too much experimenter intervention, or a small adjustment that didn't really matter?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnASWnu4qYnr"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2WXe3tvqzkv"
      },
      "source": [
        "12) Examine which games the network achieved better-than-human performance on, constrasting with those where the human expert did better.  (Enduro is a racing game, and Q-Bert is a game where the player must walk over all the squares on the board while avoiding monsters.)  What does this say about the tasks that the network is good and bad at?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhKtaHwAtT0k"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ17V0xktxoS"
      },
      "source": [
        "13) How directly would you be able to apply this technology to training self-driving cars?  What would you need in addition to what's described here?  Is it, in your opinion, a straightforward adaptation or a long way to go?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0DLotGRuMMY"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwCdgNqnekqW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
