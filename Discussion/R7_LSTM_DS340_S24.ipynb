{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUAk2V8wY_MW"
      },
      "source": [
        "#Discussion Question\n",
        "Several forms of machine learning that involve time, including vanilla RNNs and LSTMs, are rather better at reacting to the recent past than the distant past.  What are some tasks humans perform where short-term memory is all you need to succeed?  Are there any tasks that benefit from long-term memory?  What does this say about the roles AIs might play in society if they are much better at reacting to the present than the past?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN3JQOjqRwKc"
      },
      "source": [
        "# LSTMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kriSCXeHRwKr"
      },
      "source": [
        "This exercise is based on the example at: https://keras.io/examples/generative/lstm_character_level_text_generation/.  It also borrows some ideas from the code in *Learning Deep Learning* by Magnus Ekman, which appears in the lecture slides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4FJUU6kRwLA"
      },
      "source": [
        "We're going to complete the missing parts of an LSTM that predicts the next characters in some text.  This is the same task as the LSTM in lecture, but we take a different approach in places.  Our demo uses Mary Shelley's Frankenstein (via Project Gutenberg, www.gutenberg.org) as a training corpus.  This is rather short, but has the advantage of not taking too long to train during section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_to-zeqjRwLF"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Npie78KpRwLL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus length: 441034\n"
          ]
        }
      ],
      "source": [
        "with io.open(\"frankenstein.txt\", encoding=\"utf-8\") as f:\n",
        "    text = f.read().lower()\n",
        "text = text.replace(\"\\n\", \" \")  # We remove newlines chars for nicer display\n",
        "print(\"Corpus length:\", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r8cqgzeCiyOx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chars: 57\n"
          ]
        }
      ],
      "source": [
        "# Make lookup tables, character<->index\n",
        "chars = sorted(list(set(text)))\n",
        "print(\"Total chars:\", len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdXA4zEJitwq"
      },
      "source": [
        "You'll now write 2 functions to practice turning a time series into training data for an LSTM.\n",
        "\n",
        "**make_sequences(text, seqlen, step)**:  This function should return two lists.  The first list should contain subsequences of *text*.  These should each have *seqlen* characters that were contiguous in the original string.  Each subsequence should start *step* characters after the last.  So make_sequences('example!', 3, 2) should return ['exa','amp', 'ple'] as the first return value.  For each string in the first return value, the corresponding string in the second return value should be the character that comes next:  ['m', 'l', '!'] in the example.  (Stop iterating through the text when there aren't enough characters for a next character in the second list.)\n",
        "\n",
        "**to_one_hot(seqs, nexts, char_indices)**: Returns two matrices, X and y.  X is a 3D array where X[i,j,:] is a one-hot encoding of the jth character of sequence i (so 1 at the right character index and 0 elsewhere). y is a 2D array where y[i,:] is a one-hot encoding of nexts[i].  char_indices is assumed to be the dictionary of the same name created earlier.  (For efficiency, pass dtype=bool to your arrays upon creation.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4PPwK6M2lolM"
      },
      "outputs": [],
      "source": [
        "def make_sequences(text, seqlen, step):\n",
        "  # TODO\n",
        "    sequences = []\n",
        "    next_chars = []\n",
        "    for i in range(0, len(text) - seqlen, step):\n",
        "        sequences.append(text[i: i + seqlen])\n",
        "        next_chars.append(text[i + seqlen])\n",
        "    return sequences, next_chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "esrp9pOjnWbZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['exa', 'amp', 'ple'], ['m', 'l', '!'])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "make_sequences('example!', 3, 2)\n",
        "#output: (['exa', 'amp', 'ple'], ['m', 'l', '!'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FWfsVjRWnknw"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(seqs, nexts, char_indices):\n",
        "  # TODO\n",
        "    # Number of sequences\n",
        "    num_seqs = len(seqs)\n",
        "    # Length of each sequence\n",
        "    seqlen = len(seqs[0])\n",
        "    # Number of unique characters\n",
        "    num_chars = len(char_indices)\n",
        "\n",
        "    # Initialize the 3D array X for input sequences with dimensions:\n",
        "    # number of sequences x length of each sequence x number of unique characters\n",
        "    X = np.zeros((num_seqs, seqlen, num_chars), dtype=bool)\n",
        "\n",
        "    # Initialize the 2D array y for next characters with dimensions:\n",
        "    # number of sequences x number of unique characters\n",
        "    y = np.zeros((num_seqs, num_chars), dtype=bool)\n",
        "\n",
        "    # Fill in the one-hot encoded arrays\n",
        "    for i, seq in enumerate(seqs):\n",
        "        for j, char in enumerate(seq):\n",
        "            index = char_indices[char]  # Find the index of the character\n",
        "            X[i, j, index] = 1  # Set the corresponding position in X to 1\n",
        "        next_char_index = char_indices[nexts[i]]  # Find the index of the next character\n",
        "        y[i, next_char_index] = 1  # Set the corresponding position in y to 1\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0yXfB6N0oeBE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[[False, False,  True, False, False, False, False],\n",
              "         [False, False, False, False, False, False,  True],\n",
              "         [False,  True, False, False, False, False, False]],\n",
              " \n",
              "        [[False,  True, False, False, False, False, False],\n",
              "         [False, False, False, False,  True, False, False],\n",
              "         [False, False, False, False, False,  True, False]],\n",
              " \n",
              "        [[False, False, False, False, False,  True, False],\n",
              "         [False, False, False,  True, False, False, False],\n",
              "         [False, False,  True, False, False, False, False]]]),\n",
              " array([[False, False, False, False,  True, False, False],\n",
              "        [False, False, False,  True, False, False, False],\n",
              "        [ True, False, False, False, False, False, False]]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a tiny dict for testing purposes\n",
        "test_chars = sorted(list(set('example!')))\n",
        "test_char_indices = dict((c, i) for i, c in enumerate(test_chars))\n",
        "seqs, nexts = make_sequences('example!', 3, 2)\n",
        "to_one_hot(seqs, nexts, test_char_indices)\n",
        "# Examine the one-hot encodings ... do they make sense for this example?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUSp5pVErdMi"
      },
      "source": [
        "Once you're satisfied with your functions, you can proceed to create the training data from the Frankenstein text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "y2XKF0u7rPS1"
      },
      "outputs": [],
      "source": [
        "seqlen = 40\n",
        "step = 3\n",
        "seqs, nexts = make_sequences(text, seqlen, step)\n",
        "X, y = to_one_hot(seqs, nexts, char_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ue0mQqr5H_"
      },
      "source": [
        "Once the data is in the right format, there's not much to creating a basic LSTM that can train from it and make predictions.  We've omitted just the last layer, the output layer, from the LSTM-based neural network below.  Can you figure out what it should be?  Hint:  the output is a choice of letter, again in the one-hot encoding set up earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WrIbgmS0RwLM"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Assuming 'chars' is your set of unique characters\n",
        "num_unique_chars = len(chars)\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(seqlen, num_unique_chars)),\n",
        "        layers.LSTM(128),\n",
        "        layers.Dense(num_unique_chars, activation='softmax'),  # The corrected last layer\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Use the legacy RMSprop optimizer due to the warning about performance on M1/M2 Macs\n",
        "optimizer = keras.optimizers.legacy.RMSprop(learning_rate=0.01)  # Updated for compatibility\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NssAvvfuzPHY"
      },
      "source": [
        "Now go ahead and train the LSTM.  You may want to do this in Google Colab with GPU acceleration (Edit->Notebook settings), unless you have a fast GPU of your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XZdP9lwPRwLb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1149/1149 [==============================] - 19s 17ms/step - loss: 1.5958\n",
            "\n",
            "Generating text after epoch: 0\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"rk (any work on which the phrase \"projec\"\n",
            "...Generated:  t gutenberg-tm.  on the same hate a some and mine \n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"rk (any work on which the phrase \"projec\"\n",
            "...Generated:  t gutingsts he was or perits, repsuding my was or \n",
            "\n",
            "1149/1149 [==============================] - 19s 17ms/step - loss: 1.4819\n",
            "\n",
            "Generating text after epoch: 1\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \" head of the mourner.  the deep grief wh\"\n",
            "...Generated:  ich i was part of life of dear that feeling the su\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \" head of the mourner.  the deep grief wh\"\n",
            "...Generated:  ich de assupanions, but a worment teasood being.  \n",
            "\n",
            "1149/1149 [==============================] - 19s 17ms/step - loss: 1.4225\n",
            "\n",
            "Generating text after epoch: 2\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"of the youthful lovers, while in his hea\"\n",
            "...Generated:  rt of my power of the weaked to any diffure, i cou\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"of the youthful lovers, while in his hea\"\n",
            "...Generated:  lt, i was not monthration.  to the earth.  \"my fat\n",
            "\n",
            "1149/1149 [==============================] - 19s 17ms/step - loss: 1.3817\n",
            "\n",
            "Generating text after epoch: 3\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"every here and there peeping forth from \"\n",
            "...Generated:  my hands would have sunder for the horred strongel\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"every here and there peeping forth from \"\n",
            "...Generated:  me infuonyd ppection!\"  \"fountixt; had accered is \n",
            "\n",
            "1149/1149 [==============================] - 20s 18ms/step - loss: 1.3535\n",
            "\n",
            "Generating text after epoch: 4\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \" wont to meet.  how i have lived i hardl\"\n",
            "...Generated:  y where a con one remored parts of an arrive the h\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \" wont to meet.  how i have lived i hardl\"\n",
            "...Generated:  es and nother, dishatito and sefectime to be facti\n",
            "\n",
            "1149/1149 [==============================] - 21s 18ms/step - loss: 1.3297\n",
            "\n",
            "Generating text after epoch: 5\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"ure bade me weep no more.  then again th\"\n",
            "...Generated:  e sea had varte and presenting the watery in the f\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"ure bade me weep no more.  then again th\"\n",
            "...Generated:  e nelsoven tigast that who only be barked.  wher t\n",
            "\n",
            "1149/1149 [==============================] - 20s 17ms/step - loss: 1.3121\n",
            "\n",
            "Generating text after epoch: 6\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"r me.\" his voice seemed suffocated, and \"\n",
            "...Generated:  such mind interest in my howing to deferent return\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"r me.\" his voice seemed suffocated, and \"\n",
            "...Generated:  i libe of morite.  you shudied hopes sasded any in\n",
            "\n",
            "1149/1149 [==============================] - 19s 17ms/step - loss: 1.2996\n",
            "\n",
            "Generating text after epoch: 7\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"less and nervous.  every moment i feared\"\n",
            "...Generated:   my companion in my eyesed the cottage of the stra\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"less and nervous.  every moment i feared\"\n",
            "...Generated:   to vessige from my fancy apprehence from which i \n",
            "\n",
            "1149/1149 [==============================] - 19s 17ms/step - loss: 1.2840\n",
            "\n",
            "Generating text after epoch: 8\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"morse will not cease to rankle in my wou\"\n",
            "...Generated:  ntain, i am his huttered and fearing that are the \n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"morse will not cease to rankle in my wou\"\n",
            "...Generated:  ntains.  ewalice, and i may velligion. , not with \n",
            "\n",
            "1149/1149 [==============================] - 19s 17ms/step - loss: 1.2760\n",
            "\n",
            "Generating text after epoch: 9\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"vine benignity to one expressive of disg\"\n",
            "...Generated:  ustion besided in the sea of my condisions and sev\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"vine benignity to one expressive of disg\"\n",
            "...Generated:  ubess in the hight than the deed, that me you have\n",
            "\n",
            "1149/1149 [==============================] - 19s 17ms/step - loss: 1.2659\n",
            "\n",
            "Generating text after epoch: 10\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"me with kindness.  how sweet is the affe\"\n",
            "...Generated:  ction is to the any own promise and of the must di\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"me with kindness.  how sweet is the affe\"\n",
            "...Generated:  ction wcentised and lifeforty wurking and locked t\n",
            "\n",
            "1149/1149 [==============================] - 20s 17ms/step - loss: 1.2582\n",
            "\n",
            "Generating text after epoch: 11\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"hall not curse my maker.\"  his words had\"\n",
            "...Generated:   as the cour passed on the stranger and death in t\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"hall not curse my maker.\"  his words had\"\n",
            "...Generated:   he recompenced to lifely from at the familially f\n",
            "\n",
            "1149/1149 [==============================] - 20s 17ms/step - loss: 1.2490\n",
            "\n",
            "Generating text after epoch: 12\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \" fill my hearer with consternation and m\"\n",
            "...Generated:  y prepare to present and from my father and was am\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \" fill my hearer with consternation and m\"\n",
            "...Generated:  e replied to which shaund amusipive dors of look a\n",
            "\n",
            "1149/1149 [==============================] - 19s 17ms/step - loss: 1.2439\n",
            "\n",
            "Generating text after epoch: 13\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"cret.  yet, still, words like those i ha\"\n",
            "...Generated:  d not such some man eng and country, and i returne\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"cret.  yet, still, words like those i ha\"\n",
            "...Generated:  n terning the hutany of these susten his destroy t\n",
            "\n",
            "1149/1149 [==============================] - 19s 17ms/step - loss: 1.2374\n",
            "\n",
            "Generating text after epoch: 14\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"ne among the schiavi ognor frementi, who\"\n",
            "...Generated:   was death and manner engaged from the death of hi\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"ne among the schiavi ognor frementi, who\"\n",
            "...Generated:  se deshan do, and i was a disevely.  but the sight\n",
            "\n",
            "1149/1149 [==============================] - 20s 17ms/step - loss: 1.2309\n",
            "\n",
            "Generating text after epoch: 15\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"stantly informed me that it was the wret\"\n",
            "...Generated:  chedness of the languished on the recome by the fa\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"stantly informed me that it was the wret\"\n",
            "...Generated:  ched?  fom againtring read saved a feet aegel rest\n",
            "\n",
            "1149/1149 [==============================] - 20s 17ms/step - loss: 1.2265\n",
            "\n",
            "Generating text after epoch: 16\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"license included with this ebook or onli\"\n",
            "...Generated:  ne and then being a thes yougones to way and persu\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"license included with this ebook or onli\"\n",
            "...Generated:  ess?  but have his the but the vail fan come and n\n",
            "\n",
            "1149/1149 [==============================] - 19s 17ms/step - loss: 1.2205\n",
            "\n",
            "Generating text after epoch: 17\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"gun and many pistols, putting to flight \"\n",
            "...Generated:  with which i returned, and i tramely shall and the\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"gun and many pistols, putting to flight \"\n",
            "...Generated:  many convey the adgutata.  not that my pass hillor\n",
            "\n",
            "1149/1149 [==============================] - 19s 17ms/step - loss: 1.2151\n",
            "\n",
            "Generating text after epoch: 18\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"rrived from paris.  \"the government of f\"\n",
            "...Generated:  ear that the abhorr of the stranger and who entran\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"rrived from paris.  \"the government of f\"\n",
            "...Generated:  ood sembles whose mad him, laps, and the day and a\n",
            "\n",
            "1149/1149 [==============================] - 19s 17ms/step - loss: 1.2130\n",
            "\n",
            "Generating text after epoch: 19\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \" its worst shape.  one day, when my fath\"\n",
            "...Generated:  er and the family the passeon the strange and acce\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \" its worst shape.  one day, when my fath\"\n",
            "...Generated:  er without the dablers of the first appean of math\n",
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "batch_size = 128\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # Scale the logits before applying softmax\n",
        "    if temperature <= 0:  # Prevent division by zero\n",
        "        temperature = 1e-10\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds + 1e-10) / temperature  # Add a tiny number to prevent log(0)\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    \n",
        "    # Draw an index based on the probability distribution\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.fit(X, y, batch_size=batch_size, epochs=1)\n",
        "    print()\n",
        "    print(\"Generating text after epoch: %d\" % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - seqlen - 1)\n",
        "    for temp in [0.5, 1.0]:\n",
        "        print(\"...Temperature:\", temp)\n",
        "\n",
        "        generated = \"\"\n",
        "        sentence = text[start_index : start_index + seqlen]\n",
        "        print('...Generating with seed: \"' + sentence + '\"')\n",
        "\n",
        "        for i in range(50):\n",
        "            x_pred = np.zeros((1, seqlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.0\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, temp)\n",
        "            next_char = indices_char[next_index]\n",
        "            sentence = sentence[1:] + next_char\n",
        "            generated += next_char\n",
        "\n",
        "        print(\"...Generated: \", generated)\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN0eHBPRhEcB"
      },
      "source": [
        "You might still be getting some nonsense at epoch 20, although it should be noticeably better than at the start.  This setup for training was chosen with speed of training as the foremost concern, since LSTMs can take a long time to train.  You can provide a longer training corpus - there are rather longer books available in plain text at Project Gutenberg. You can run for more epochs - Chollet, the original author of this example, says 20 is a bare minimum and 40 is recommended.  Or you can try augmenting the LSTM architecture with another layer; there's an example of this in the lecture slides.  (Be sure to set return_sequences to True for the lower LSTM layer.)\n",
        "\n",
        "Try one of these approaches, and compare your results with a neighbor who chose a different one."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
