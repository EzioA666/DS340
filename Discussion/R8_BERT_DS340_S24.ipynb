{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "JwqmHQdWePl_",
      "metadata": {
        "id": "JwqmHQdWePl_"
      },
      "source": [
        "**Discussion Question:**\n",
        "\n",
        "In a 2017 debate, Gary Marcus argued that most of the accomplishments of neural networks are on perception problems, with a few exceptions like game-playing and media generation.  \"Sheer bottom-up statistics hasn't gotten us very far on an important set of problems -- language, reasoning, planning, and common sense...\" (quoted in Genius Makers, Metz 2021 p. 269).  Share an anecdote about ChatGPT that either demonstrates facility or lack of facility with one of these traits.  What are its weak points?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e41e74",
      "metadata": {
        "id": "65e41e74"
      },
      "source": [
        "In this discussion section, you'll have a chance to play with BERT, the Google-developed large language model that is often used in NLP projects because it's freely available and a good size, neither too large nor too small.\n",
        "\n",
        "The task will remain the same from the \"Bayesian Tomatoes\" Naive Bayes task: sentiment\n",
        "analysis. We're going to change the corpus slightly; we used SST1 there, a sentiment dataset with multiple classification categories and a tree structure to the sentiment classifications that we ignored (hence all the skipped lines of the dataset). This time, we'll use SST2, a sentiment dataset collected later that has just binary sentiment classifications. The solution code for assignment 3 gets 71.8% accuracy on this dataset, as a benchmark for your other results. (Which should all be better than that.)\n",
        "\n",
        "The goal of this exercise will be to have a function that takes a sentence and outputs\n",
        "whether the sentiment is negative (0) or positive (1), using a light classifier trained on top of BERT's output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ace07a7",
      "metadata": {
        "id": "2ace07a7"
      },
      "source": [
        "For the first part of the exercise, you'll need to install the \"transformers\" module, and possibly \"torch\" as well.  The examples here happen to use PyTorch instead of TensorFlow/Keras, so if you've never worked with PyTorch, it's recommended you work in Google Colab where PyTorch is already installed instead of taking the time now to install PyTorch on your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38286941",
      "metadata": {
        "id": "38286941"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7008cb7",
      "metadata": {
        "id": "b7008cb7"
      },
      "source": [
        "Next, use the functions provided below to follow these steps (based on jalammar's\n",
        "introduction to BERT on GitHub, https://github.com/jalammar/jalammar.github.io/blob/master/\n",
        "notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb). You shouldn't\n",
        "need to copy any code from there - just call the provided functions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f91b1622",
      "metadata": {
        "id": "f91b1622"
      },
      "source": [
        "a) Download the 2000 sentiment-labeled sentences to a Pandas dataframe.\n",
        "\n",
        "b) Create a BERT tokenizer and turn the dataset's sentences into BERT tokens.\n",
        "\n",
        "c) Pad the tokens with zeros, so that the sentences are all the same length.\n",
        "\n",
        "d) Call get_bert_sentence_vectors() to extract the vectors corresponding to the first token, CLS, for each sentence. This vector is trained to represent the overall meaning of the sentence for classification tasks as best as possible.  Name your array of vectors \"vecs.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bfeab959",
      "metadata": {
        "id": "bfeab959"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import torch\n",
        "import transformers as ppb\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d48e67ee",
      "metadata": {
        "id": "d48e67ee"
      },
      "outputs": [],
      "source": [
        "# Location of SST2 sentiment dataset\n",
        "SST2_LOC = 'https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv'\n",
        "WEIGHTS = 'distilbert-base-uncased'\n",
        "# Performance on whole 6920 sentence set is very similar, but takes rather longer\n",
        "SET_SIZE = 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "14ba7ea0",
      "metadata": {
        "id": "14ba7ea0"
      },
      "outputs": [],
      "source": [
        "# Download the dataset from its Github location, return as a Pandas dataframe\n",
        "def get_dataframe():\n",
        "    df = pd.read_csv(SST2_LOC, delimiter='\\t', header=None)\n",
        "    return df[:SET_SIZE]\n",
        "\n",
        "# Extract just the labels from the dataframe\n",
        "def get_labels(df):\n",
        "    return df[1]\n",
        "\n",
        "# Get a trained tokenizer for use with BERT\n",
        "def get_tokenizer():\n",
        "    return ppb.DistilBertTokenizer.from_pretrained(WEIGHTS)\n",
        "\n",
        "# Convert the sentences into lists of tokens\n",
        "def get_tokens(dataframe, tokenizer):\n",
        "    return dataframe[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "\n",
        "# We want the sentences to all be the same length; pad with 0's to make it so\n",
        "def pad_tokens(tokenized):\n",
        "    max_len = 0\n",
        "    for i in tokenized.values:\n",
        "        if len(i) > max_len:\n",
        "            max_len = len(i)\n",
        "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "    return padded\n",
        "\n",
        "# Grab a trained DistiliBERT model\n",
        "def get_model():\n",
        "    return ppb.DistilBertModel.from_pretrained(WEIGHTS)\n",
        "\n",
        "# This step takes a little while, since it actually runs the model on all sentences.\n",
        "# Get model with get_model(), 0-padded token lists with pad_tokens() on get_tokens().\n",
        "# Only returns the [CLS] vectors representing the whole sentence, corresponding to first token.\n",
        "def get_bert_sentence_vectors(model, padded_tokens):\n",
        "    # Mask the 0's padding from attention - it's meaningless\n",
        "    mask = torch.tensor(np.where(padded_tokens != 0, 1, 0))\n",
        "    with torch.no_grad():\n",
        "        word_vecs = model(torch.tensor(padded_tokens).to(torch.int64), attention_mask=mask)\n",
        "    # First vector is for [CLS] token, represents the whole sentence\n",
        "    return word_vecs[0][:,0,:].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45f64011",
      "metadata": {
        "id": "45f64011"
      },
      "outputs": [],
      "source": [
        "# TODO use the above functions to get an array of sentence vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_e_XQjmY5G-D",
      "metadata": {
        "id": "_e_XQjmY5G-D"
      },
      "source": [
        "If you called the above functions in the right order and named your output \"vecs\", you should now have 2000 vectors representing the 2000 sentences, each with 768 elements in the vector.  You can check this below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d28dff8",
      "metadata": {
        "id": "8d28dff8"
      },
      "outputs": [],
      "source": [
        "vecs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0vlIL1ra4lkT",
      "metadata": {
        "id": "0vlIL1ra4lkT"
      },
      "source": [
        "Now let's check that sentences with similar meanings have vectors that are close in the space.  Write a function find_closest_sentences(vecs, sentences) that finds the two different sentences in the data that have the most similar meaning (the distance between the vectors is smallest).\n",
        "\n",
        "(The sentences are the zeroth element of the SST2 dataframe, df[0]. It may take a little while. For speed, try using numpy.linalg.norm instead of your own implementation of distance.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2LZ48XDT5xiZ",
      "metadata": {
        "id": "2LZ48XDT5xiZ"
      },
      "outputs": [],
      "source": [
        "# Find closest sentences:  Find the two sentences closest in the space\n",
        "# (vecs from BERT, sentences in same order from df[0])\n",
        "def find_closest_sentences(vecs, sentences):\n",
        "    # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_SN9BkAe525F",
      "metadata": {
        "id": "_SN9BkAe525F"
      },
      "outputs": [],
      "source": [
        "find_closest_sentences(vecs, df[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WODCON1A6l_A",
      "metadata": {
        "id": "WODCON1A6l_A"
      },
      "source": [
        "Now use scikit-learn and train_test_split() to separate these 2000 sentences and labels into a train and test set (80%/20%), and then pick your favorite scikit-learn classifier to train on the training data semantic vectors to predict the sentiment, testing on your test data.  (Some options include k-nearest-neighbors, Adaboost, random forests, or a simple neural network or \"multilayer perceptron\" as scikit-learn calls it.)  Recall that the function names you want are yourmodel.fit(train_features,train_labels) and yourmodel.score(test_features, test_labels).  Compare your results with other students who tried different classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8jiqzfuh7Qwa",
      "metadata": {
        "id": "8jiqzfuh7Qwa"
      },
      "outputs": [],
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(vecs, labels,test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbsG7IlS6XO7",
      "metadata": {
        "id": "tbsG7IlS6XO7"
      },
      "outputs": [],
      "source": [
        "# TODO:  train and test a classifier on the features here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
