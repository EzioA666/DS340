{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rN_JM1unpNOR"
   },
   "source": [
    "# Decision trees (60 points total)\n",
    "\n",
    "Below is the code for a decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "heZhaO8aMKnL"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy.stats\n",
    "import random\n",
    "from scipy.stats import chi2_contingency\n",
    "from random import sample\n",
    "\n",
    "\n",
    "ENABLE_PRUNING = False\n",
    "RANDOM_FOREST = False\n",
    "PRUNE_THRESHOLD = 0.05  # Threshold for p-value in chi-square test for pruning\n",
    "RANDOM_DECISIONS_RATIO = 0.3  # Fraction of decisions to consider if RANDOM_FOREST is True\n",
    "\n",
    "class DecisionTree:\n",
    "  \"\"\" A decision tree for machine learning.  Since it's a tree,\n",
    "  it's defined as a node with possible subtrees as children.\n",
    "\n",
    "  self.leaf (boolean):  Whether the node is a leaf (no children).\n",
    "  self.outcome (boolean):  If this is a leaf, its recommended classification of\n",
    "     a classified example that ends up there.\n",
    "  self.decision (Decision):  If this isn't a leaf, the decision represented by\n",
    "     the node.  See Decision class below.\n",
    "  self.yes (DecisionTree):  The subtree followed if an example answers \"yes\"\n",
    "     to the decision.\n",
    "  self.no (DecisionTree):  The subtree followed if an example answers \"no\"\n",
    "     to the decision.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, outcome):\n",
    "    \"\"\"A constructor for a leaf.\"\"\"\n",
    "    self.leaf = True\n",
    "    self.outcome = outcome\n",
    "    self.decision = None\n",
    "    self.yes = None\n",
    "    self.no = None\n",
    "\n",
    "  def __init__(self, decision, yes, no):\n",
    "    \"\"\"A constructor for an interior node.\"\"\"\n",
    "    self.leaf = False\n",
    "    self.decision = decision\n",
    "    self.yes = yes\n",
    "    self.no = no\n",
    "\n",
    "  # Examples are assumed to be a list-of-lists with each list\n",
    "  # an example.\n",
    "  def __init__(self, examples, labels):\n",
    "    \"\"\"A recursive constructor for building the tree from examples.\"\"\"\n",
    "    agree, label = all_agree(labels)\n",
    "    if (agree):\n",
    "      self.leaf = True\n",
    "      self.outcome = label\n",
    "      self.decision = None\n",
    "      self.yes = None\n",
    "      self.no = None\n",
    "      return\n",
    "    all_decisions = generate_decisions(examples)\n",
    "    if RANDOM_FOREST:\n",
    "        decisions_sample_size = int(math.sqrt(len(all_decisions)))\n",
    "        all_decisions = random.sample(list(all_decisions), decisions_sample_size)\n",
    "    best_decision = None\n",
    "    best_entropy = 1\n",
    "    best_split = None\n",
    "    for decision in all_decisions:\n",
    "      split = split_by_decision(decision, examples, labels)\n",
    "      expected_entropy = try_split(split)\n",
    "      if expected_entropy < best_entropy:\n",
    "        best_decision = decision\n",
    "        best_entropy = expected_entropy\n",
    "        best_split = split\n",
    "    # Check whether nothing improved - we didn't split\n",
    "    if best_split == None or len(best_split.yes_examples) == 0 or len(best_split.no_examples) == 0:\n",
    "      self.leaf = True\n",
    "      self.outcome = majority(labels)\n",
    "      self.decision = None\n",
    "      self.yes = None\n",
    "      self.no = None\n",
    "      return\n",
    "\n",
    "    self.leaf = False\n",
    "    self.outcome = None\n",
    "    self.decision = best_decision\n",
    "    self.yes = DecisionTree(best_split.yes_examples, best_split.yes_labels)\n",
    "    self.no = DecisionTree(best_split.no_examples, best_split.no_labels)\n",
    "    if ENABLE_PRUNING and self.prune(best_split):\n",
    "      self.leaf = True\n",
    "      self.outcome = majority(labels)\n",
    "      self.decision = None\n",
    "      self.yes = None\n",
    "      self.no = None\n",
    "    return\n",
    "\n",
    "  def __str__(self):\n",
    "    return recursive_string(self, 0)\n",
    "\n",
    "  # TODO\n",
    "  def prune(self, best_split):\n",
    "    \"\"\"No effect (return False) unless both children are leaves.\n",
    "    If they are, return True if a chi-square test between feature\n",
    "    and label is not significant -- the caller will prune the node,\n",
    "    turning it into a leaf.\"\"\"\n",
    "    \"\"\"Implements pruning using a chi-square test.\"\"\"\n",
    "    if not self.yes.leaf or not self.no.leaf:\n",
    "        # If either child is not a leaf, do not prune\n",
    "        return False\n",
    "    \n",
    "    # Calculate counts for the chi-square test\n",
    "    yes_label_count = sum(best_split.yes_labels)\n",
    "    no_label_count = sum(best_split.no_labels)\n",
    "    yes_feature_count = len(best_split.yes_labels)\n",
    "    no_feature_count = len(best_split.no_labels)\n",
    "    \n",
    "    # Counts for [[featureANDlabel, featureANDNOTlabel],[NOTfeatureANDlabel,NOTfeatureANDNOTlabel]]\n",
    "    feature_and_label = yes_label_count\n",
    "    feature_and_not_label = yes_feature_count - yes_label_count\n",
    "    not_feature_and_label = no_label_count\n",
    "    not_feature_and_not_label = no_feature_count - no_label_count\n",
    "    \n",
    "    contingency_table = [[feature_and_label, feature_and_not_label], [not_feature_and_label, not_feature_and_not_label]]\n",
    "    \n",
    "    # Perform chi-square test\n",
    "    _, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "    \n",
    "    # If p-value < 0.05, the feature and label are considered dependent, and we should not prune\n",
    "    # If p-value >= 0.05, they are independent, and we can prune\n",
    "    return p_value >= 0.05\n",
    "\n",
    "  def classify(self, example):\n",
    "    \"\"\"Recursively decides how this tree would classify the passed example.\"\"\"\n",
    "    if self.leaf:\n",
    "      return self.outcome\n",
    "    if self.decision.applies_to(example):\n",
    "      return self.yes.classify(example)\n",
    "    return self.no.classify(example)\n",
    "\n",
    "def recursive_string(tree, indent):\n",
    "    \"\"\"Recursively print the tree with an indentation corresponding to\n",
    "    tree depth.  Useful for debugging.  Is also the __str__() implementation.\"\"\"\n",
    "    if (tree.leaf):\n",
    "      return '  ' * indent + str(tree.outcome) + '\\n'\n",
    "    else:\n",
    "      mystr = '  ' * indent + 'if ' + str(tree.decision) + ':\\n'\n",
    "      mystr += recursive_string(tree.yes, indent+1)\n",
    "      mystr += recursive_string(tree.no, indent+1)\n",
    "      return mystr\n",
    "\n",
    "# Assume numerical features for convenience\n",
    "class Decision:\n",
    "  \"\"\"Object representing a decision to make about an example.  Each interior\n",
    "  node of the tree has one of these.\n",
    "  feature_num:  Index into which feature is being used for the decision.\n",
    "  thresh:  For features that are numeric, the numerical threshold for returning True.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, feature_num, thresh):\n",
    "    self.feature_num = feature_num\n",
    "    self.thresh = thresh\n",
    "\n",
    "  def applies_to(self, example):\n",
    "    \"\"\"Returns true if the example should follow the \"yes\" branch for the decision.\"\"\"\n",
    "    if example[self.feature_num] >= self.thresh:\n",
    "      return True\n",
    "    return False\n",
    "  \n",
    "  def __str__(self):\n",
    "    return \"Feature \" + str(self.feature_num) + \" >= \" + str(self.thresh)\n",
    "\n",
    "# Split carries yes examples, yes labels, no examples, no labels\n",
    "# for convenience\n",
    "class Split:\n",
    "  \"\"\"If a Decision would separate the examples into two piles, then a Split\n",
    "  represents those two piles.\n",
    "\n",
    "  yes_examples(list-of-lists):  The examples that would satisfy the Decision.\n",
    "  yes_labels(list of bools):  The labels on the yes_examples.\n",
    "  no_examples(list-of-lists):  The examples that don't satisfy the Decision.\n",
    "  no_labels(list of bools):  The labels of the no_examples.\"\"\"\n",
    "  def __init__(self, yes_examples, yes_labels, no_examples, no_labels):\n",
    "    self.yes_examples = yes_examples\n",
    "    self.yes_labels = yes_labels\n",
    "    self.no_examples = no_examples\n",
    "    self.no_labels = no_labels\n",
    "  \n",
    "  # For debugging\n",
    "  def __str__(self):\n",
    "    out = str(self.yes_examples) + '\\n'\n",
    "    out += str(self.yes_labels) + '\\n'\n",
    "    out += str(self.no_examples) + '\\n'\n",
    "    out += str(self.no_labels) + '\\n'\n",
    "    return out\n",
    "\n",
    "def majority(labels):\n",
    "  \"\"\"Determine whether the majority of the labels is 1 (return True) or 0 (False).\"\"\"\n",
    "  yes_count = sum(labels)\n",
    "  if yes_count >= len(labels)/2:\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def all_agree(labels):\n",
    "  \"\"\"First return value is whether all the labels are the same.\n",
    "  Second return value is the majority classification of the labels.\"\"\"\n",
    "  return (sum(labels) == len(labels)) or (sum(labels) == 0), majority(labels)\n",
    "\n",
    "def generate_decisions(examples):\n",
    "  \"\"\"Given a list of examples, generate all possible Decisions based on those\n",
    "  examples' features and numerical values.  Return a list of those Decisions.\"\"\"\n",
    "  decisions = set() # Use set to avoid decision duplication\n",
    "  feature_count = len(examples[0])\n",
    "  for example in examples:\n",
    "    for j in range(feature_count):\n",
    "      decisions.add(Decision(j,example[j]))\n",
    "  return decisions\n",
    "\n",
    "def try_split(split):\n",
    "  \"\"\"Given the split of examples that did and didn't satisfy the Decision,\n",
    "  calculate the expected entropy (the criterion used to find the best Decision).\"\"\"\n",
    "  yes_entropy = entropy(split.yes_labels)\n",
    "  no_entropy = entropy(split.no_labels)\n",
    "  example_count = len(split.yes_labels) + len(split.no_labels)\n",
    "  yes_prob = len(split.yes_labels)/example_count\n",
    "  no_prob = len(split.no_labels)/example_count\n",
    "  expected = yes_prob * yes_entropy + no_prob * no_entropy\n",
    "  return expected\n",
    "\n",
    "def split_by_decision(decision, examples, labels):\n",
    "  \"\"\"Using the Decision argument, divide the examples into those that satisfy\n",
    "  the Decision and those that don't, and create a Split object to keep these\n",
    "  two piles separate.  Split the corresponding labels as well.\"\"\"\n",
    "  yes_examples = []\n",
    "  yes_labels = []\n",
    "  no_examples = []\n",
    "  no_labels = []\n",
    "  for i, example in enumerate(examples):\n",
    "    if example[decision.feature_num] >= decision.thresh:\n",
    "      yes_examples += [example]\n",
    "      yes_labels += [labels[i]]\n",
    "    else:\n",
    "      no_examples += [example]\n",
    "      no_labels += [labels[i]]\n",
    "  return Split(yes_examples, yes_labels, no_examples, no_labels)\n",
    "\n",
    "def entropy(bool_list):\n",
    "  \"\"\"Given a list of True and False values (or 0's and 1's), calculate the\n",
    "  entropy of the list.\"\"\"\n",
    "  true_count = sum(bool_list)\n",
    "  false_count = len(bool_list) - sum(bool_list)\n",
    "  if true_count == 0 or false_count == 0:\n",
    "    return 0\n",
    "  true_prob = true_count/len(bool_list)\n",
    "  false_prob = false_count/len(bool_list)\n",
    "  return - true_prob * math.log(true_prob, 2) - false_prob * math.log(false_prob,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CtAT5JCFOBa"
   },
   "source": [
    "Upload 'adult2000.csv' with the following code.  This is census data where the target variable to predict is whether the person made $50K/year or more.  (We're just using the first 2000 entries for speed reasons.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KMyohpm_s5MU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ubzHidaCtC6L"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  education  education-num      marital-status  \\\n",
       "0   39         State-gov  Bachelors             13       Never-married   \n",
       "1   50  Self-emp-not-inc  Bachelors             13  Married-civ-spouse   \n",
       "2   38           Private    HS-grad              9            Divorced   \n",
       "3   53           Private       11th              7  Married-civ-spouse   \n",
       "4   28           Private  Bachelors             13  Married-civ-spouse   \n",
       "\n",
       "          occupation   relationship   race     sex  capital-gain  \\\n",
       "0       Adm-clerical  Not-in-family  White    Male          2174   \n",
       "1    Exec-managerial        Husband  White    Male             0   \n",
       "2  Handlers-cleaners  Not-in-family  White    Male             0   \n",
       "3  Handlers-cleaners        Husband  Black    Male             0   \n",
       "4     Prof-specialty           Wife  Black  Female             0   \n",
       "\n",
       "   capital-loss  hours-per-week native-country  Target  \n",
       "0             0              40  United-States       0  \n",
       "1             0              13  United-States       0  \n",
       "2             0              40  United-States       0  \n",
       "3             0              40  United-States       0  \n",
       "4             0              40           Cuba       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('adult2000.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRwvipwoFgc7"
   },
   "source": [
    "The \"Target\" column has our labels, whether the individual made $50K/year or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2ND3_GbrtsvK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "1994    1\n",
       "1995    0\n",
       "1996    1\n",
       "1997    0\n",
       "1998    1\n",
       "Name: Target, Length: 1999, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df[\"Target\"]\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "larDeI7LX4Ig"
   },
   "source": [
    "Since the decision tree code only works with numerical data, we can turn the string data into numerical data using \"one-hot encoding\".  We make a new column for each possible value of the categorical data, and use True and False values for that column, which are interpreted later in the code as 1's and 0's.  (Note:  if you are frustrated by how long it takes to train your decision tree, you could temporarily skip this cell when loading the data and just use num_features in the cell that follows.  Be sure to revert this before turning the assignment in.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vdwrdwX7X4Ii"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>...</th>\n",
       "      <th>Guatemala</th>\n",
       "      <th>China</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Yugoslavia</th>\n",
       "      <th>Peru</th>\n",
       "      <th>Outlying-US(Guam-USVI-etc)</th>\n",
       "      <th>Scotland</th>\n",
       "      <th>Trinadad&amp;Tobago</th>\n",
       "      <th>Greece</th>\n",
       "      <th>Nicaragua</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>30</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>75</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-spouse-absent</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1999 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age         workclass     education  education-num  \\\n",
       "0      39         State-gov     Bachelors             13   \n",
       "1      50  Self-emp-not-inc     Bachelors             13   \n",
       "2      38           Private       HS-grad              9   \n",
       "3      53           Private          11th              7   \n",
       "4      28           Private     Bachelors             13   \n",
       "...   ...               ...           ...            ...   \n",
       "1994   30           Private     Bachelors             13   \n",
       "1995   44           Private  Some-college             10   \n",
       "1996   49           Private       HS-grad              9   \n",
       "1997   75  Self-emp-not-inc       Masters             14   \n",
       "1998   37           Private     Bachelors             13   \n",
       "\n",
       "             marital-status         occupation   relationship   race     sex  \\\n",
       "0             Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1        Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2                  Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3        Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4        Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "...                     ...                ...            ...    ...     ...   \n",
       "1994          Never-married    Exec-managerial      Unmarried  White  Female   \n",
       "1995               Divorced  Machine-op-inspct  Not-in-family  White    Male   \n",
       "1996     Married-civ-spouse              Sales        Husband  White    Male   \n",
       "1997  Married-spouse-absent     Prof-specialty  Not-in-family  White  Female   \n",
       "1998     Married-civ-spouse              Sales        Husband  White    Male   \n",
       "\n",
       "      capital-gain  ...  Guatemala  China  Japan  Yugoslavia   Peru  \\\n",
       "0             2174  ...      False  False  False       False  False   \n",
       "1                0  ...      False  False  False       False  False   \n",
       "2                0  ...      False  False  False       False  False   \n",
       "3                0  ...      False  False  False       False  False   \n",
       "4                0  ...      False  False  False       False  False   \n",
       "...            ...  ...        ...    ...    ...         ...    ...   \n",
       "1994             0  ...      False  False  False       False  False   \n",
       "1995             0  ...      False  False  False       False  False   \n",
       "1996             0  ...      False  False  False       False  False   \n",
       "1997             0  ...      False  False  False       False  False   \n",
       "1998             0  ...      False  False  False       False  False   \n",
       "\n",
       "      Outlying-US(Guam-USVI-etc)  Scotland  Trinadad&Tobago  Greece  Nicaragua  \n",
       "0                          False     False            False   False      False  \n",
       "1                          False     False            False   False      False  \n",
       "2                          False     False            False   False      False  \n",
       "3                          False     False            False   False      False  \n",
       "4                          False     False            False   False      False  \n",
       "...                          ...       ...              ...     ...        ...  \n",
       "1994                       False     False            False   False      False  \n",
       "1995                       False     False            False   False      False  \n",
       "1996                       False     False            False   False      False  \n",
       "1997                       False     False            False   False      False  \n",
       "1998                       False     False            False   False      False  \n",
       "\n",
       "[1999 rows x 92 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot(df, colname):\n",
    "    values = df[colname].unique()\n",
    "    for value in values:\n",
    "        df[value] = df[colname] == value\n",
    "    return df\n",
    "\n",
    "one_hot(df, \"workclass\")\n",
    "one_hot(df, \"marital-status\")\n",
    "one_hot(df, \"occupation\")\n",
    "one_hot(df, \"relationship\")\n",
    "one_hot(df, \"race\")\n",
    "one_hot(df, \"sex\")\n",
    "one_hot(df, \"native-country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qfWyy70ht2za"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>State-gov</th>\n",
       "      <th>Self-emp-not-inc</th>\n",
       "      <th>Private</th>\n",
       "      <th>Federal-gov</th>\n",
       "      <th>Local-gov</th>\n",
       "      <th>...</th>\n",
       "      <th>Guatemala</th>\n",
       "      <th>China</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Yugoslavia</th>\n",
       "      <th>Peru</th>\n",
       "      <th>Outlying-US(Guam-USVI-etc)</th>\n",
       "      <th>Scotland</th>\n",
       "      <th>Trinadad&amp;Tobago</th>\n",
       "      <th>Greece</th>\n",
       "      <th>Nicaragua</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  education-num  capital-gain  capital-loss  hours-per-week  State-gov  \\\n",
       "0   39             13          2174             0              40       True   \n",
       "1   50             13             0             0              13      False   \n",
       "2   38              9             0             0              40      False   \n",
       "3   53              7             0             0              40      False   \n",
       "4   28             13             0             0              40      False   \n",
       "\n",
       "   Self-emp-not-inc  Private  Federal-gov  Local-gov  ...  Guatemala  China  \\\n",
       "0             False    False        False      False  ...      False  False   \n",
       "1              True    False        False      False  ...      False  False   \n",
       "2             False     True        False      False  ...      False  False   \n",
       "3             False     True        False      False  ...      False  False   \n",
       "4             False     True        False      False  ...      False  False   \n",
       "\n",
       "   Japan  Yugoslavia   Peru  Outlying-US(Guam-USVI-etc)  Scotland  \\\n",
       "0  False       False  False                       False     False   \n",
       "1  False       False  False                       False     False   \n",
       "2  False       False  False                       False     False   \n",
       "3  False       False  False                       False     False   \n",
       "4  False       False  False                       False     False   \n",
       "\n",
       "   Trinadad&Tobago  Greece  Nicaragua  \n",
       "0            False   False      False  \n",
       "1            False   False      False  \n",
       "2            False   False      False  \n",
       "3            False   False      False  \n",
       "4            False   False      False  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = df[[\"age\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]]\n",
    "one_hot_features = df.iloc[:, 14:]\n",
    "features = pd.concat([num_features, one_hot_features], axis=1)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vBPtu3WCuRIJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features,labels, random_state=110)\n",
    "\n",
    "features_train_list = features_train.values.tolist()\n",
    "labels_train_list = labels_train.tolist()\n",
    "features_test_list = features_test.values.tolist()\n",
    "labels_test_list = labels_test.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNLrWqqmFy4B"
   },
   "source": [
    "With list-format train/test split in hand, now it's time to train and evaluate a model.\n",
    "\n",
    "**(1) (2 points)** In the code box below, train a model on the adult2000 training data using the DecisionTree constructor we built above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EKGoon1oD7DH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree model training complete.\n",
      "Model accuracy on test data: 82.80%\n"
     ]
    }
   ],
   "source": [
    "# TODO make a decision tree!\n",
    "# Proceeding without converting labels_train and labels_test as they are already lists\n",
    "features_train_list = features_train.values.tolist()\n",
    "features_test_list = features_test.values.tolist()\n",
    "\n",
    "# Initialize and train the DecisionTree model\n",
    "model = DecisionTree(features_train_list, labels_train_list)\n",
    "print(\"Decision Tree model training complete.\")\n",
    "\n",
    "# Assuming the DecisionTree class has a method for classification (prediction)\n",
    "# Let's pretend we have a method called classify for individual predictions\n",
    "\n",
    "def evaluate_model(model, features_test, labels_test):\n",
    "    correct_predictions = 0\n",
    "    for i, test_example in enumerate(features_test):\n",
    "        prediction = model.classify(test_example)\n",
    "        if prediction == labels_test[i]:\n",
    "            correct_predictions += 1\n",
    "    accuracy = correct_predictions / len(labels_test)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model's accuracy on the test data\n",
    "accuracy = evaluate_model(model, features_test_list, labels_test_list)\n",
    "print(f\"Model accuracy on test data: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NupuvMGPHIaF"
   },
   "source": [
    "**(2) (6 points)** Code a function evaluate() that takes a trained DecisionTree, a set of examples, and a set of labels, and returns an accuracy, the number of examples it got right.  Note that it should return perfect accuracy on the test below, and should return above 90% accuracy on the training set for the adult2000 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CivdFaklyHaZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns accuracy - TODO\n",
    "def evaluate(tree, test_examples, test_labels):  \n",
    "    correct_predictions = 0\n",
    "    total_examples = len(test_examples)\n",
    "    \n",
    "    for i, example in enumerate(test_examples):\n",
    "        # Predict the label for each test example\n",
    "        predicted_label = tree.classify(example)\n",
    "        # Compare the predicted label with the actual label\n",
    "        if predicted_label == test_labels[i]:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    # Calculate accuracy as the ratio of correct predictions to total examples\n",
    "    accuracy = correct_predictions / total_examples\n",
    "    return accuracy\n",
    "\n",
    "# Test - expect perfect accuracy\n",
    "test_examples = [[0,0],[0,1],[1,0],[1,1]]\n",
    "test_labels = [1, 1, 0, 0]\n",
    "test_tree = DecisionTree(test_examples,test_labels)\n",
    "evaluate(test_tree, test_examples, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lsyy1P3HbQD"
   },
   "source": [
    "\n",
    "**(3) (4 points)** Use your evaluate function to print your trained model's accuracy for both the training data and the test data.  Then explain in your own words below why one accuracy is much higher than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "v24uE_30HhSk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Accuracy: 99.67%\n",
      "Test Data Accuracy: 82.80%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on training data\n",
    "train_accuracy = evaluate(model, features_train_list, labels_train_list)\n",
    "print(f\"Training Data Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_accuracy = evaluate(model, features_test_list, labels_test_list)\n",
    "print(f\"Test Data Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsIyK85WH2Z4"
   },
   "source": [
    "**TODO why is training score so much higher than test?**\n",
    "\n",
    "### Explaination:\n",
    "\n",
    "The discrepancy between the high training accuracy (99.67%) and lower test accuracy (82.80%) with the Decision Tree model suggests overfitting. \n",
    "\n",
    "To address overfitting and improve the model's generalization to new data, we can:\n",
    "\n",
    "1. Pruning the decision tree to remove less critical parts.\n",
    "2. Implementing cross-validation to better estimate model performance on unseen data.\n",
    "3. Using ensemble methods like Random Forests, which can reduce overfitting by averaging multiple decision trees.\n",
    "4. Adopting these strategies can help balance the model's performance, enhancing its accuracy on both training and unseen datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4, 6 pts)** The following tiny dataset seems very similar to the test dataset that we got perfect accuracy on, a couple code boxes back.  But, it doesn't seem to produce perfect accuracy.  Is it possible to design by hand a decision tree that gets perfect accuracy on this dataset?  If so, why doesn't our code succeed in automatically constructing it?  If not, why is perfect classification not possible here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's happening here?\n",
    "test_examples = [[0,0],[0,1],[1,0],[1,1]]\n",
    "test_labels = [0, 1, 1, 0]\n",
    "test_tree = DecisionTree(test_examples,test_labels)\n",
    "evaluate(test_tree, test_examples, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "\n",
    "The dataset represents an XOR logic function, which cannot be perfectly classified by a simple, linear decision tree because XOR requires a non-linear decision boundary. While it's theoretically possible to design a decision tree by hand that perfectly classifies the XOR dataset by creating a multi-level structure, automatic construction might fail due to:\n",
    "\n",
    "Linear Separability: XOR is not linearly separable with a single decision boundary, challenging for simple decision trees that rely on linear decisions at each node.\n",
    "Model Complexity: Perfectly classifying XOR requires a tree with at least two levels to capture the feature interaction, which might not be achieved if the model complexity is restricted.\n",
    "In essence, the XOR problem's complexity exceeds the capacity of basic decision trees without manual feature engineering or allowing for a sufficiently complex tree structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vkkh2LH1YwUW"
   },
   "source": [
    "**(5) (8 points)** One way to avoid overfitting in decision trees is pruning, or getting rid of decisions that aren't pulling their weight.  Complete the function prune() that returns true if the node should be pruned to be a leaf.  The function should first check whether both children are leaves; if not, the node is safe from pruning.  If both nodes are leaves, the code should check whether the best decision's feature and the label are independent according to a chi-square test, and if so, return True.  You can use the function scipy.stats.chi2_contingency(), where the four cells in the list-of-lists provided as input are counts of [[featureANDlabel, featureANDNOTlabel],[NOTfeatureANDlabel,NOTfeatureANDNOTlabel]].  (Hint:  You can compute these counts from just the yes_label and no_label lists in a Split object.)  The p-value of the contingency test (second return value) must be < 0.05 to keep the decision.\n",
    "\n",
    "When it works, you should see the train and test accuracies be more similar to each other.  The test accuracy should be a little higher or similar (randomness plays a part in the results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KZ6agRF-YtKK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8965977318212142\n",
      "0.828\n"
     ]
    }
   ],
   "source": [
    "ENABLE_PRUNING = True\n",
    "my_tree = DecisionTree(features_train_list,labels_train_list)\n",
    "print(evaluate(my_tree, features_train_list, labels_train_list))\n",
    "print(evaluate(my_tree, features_test_list, labels_test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "884_6ignDHYX"
   },
   "source": [
    "Next we'll see whether turning this into an ensemble learner helps at all.  We'll try turning the single tree into a random forest.\n",
    "\n",
    "**(6, 8 pts)** Code the function bag(), which takes a list of N examples and N labels and returns a list of N examples and N corresponding labels that have been sampled with replacement from the original lists.  (You'll find random.randrange() helpful for this part.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-3PgAH0ZFtVW"
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "def bag(examples, labels):\n",
    "    new_examples = []\n",
    "    new_labels = []\n",
    "    N = len(examples)\n",
    "    \n",
    "    for _ in range(N):\n",
    "        index = randrange(N)  # Randomly select an index\n",
    "        new_examples.append(examples[index])  # Add the example at that index to the new list\n",
    "        new_labels.append(labels[index])  # Add the corresponding label to the new list\n",
    "    \n",
    "    return new_examples, new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAkaVqW7HRCr"
   },
   "source": [
    "\n",
    "**(7, 7 pts)** Code a RandomForest constructor that takes the lists of N examples and N labels, as well as an argument for the number of random trees, and creates the list of decision trees that could be used to vote on the correct classification.  You don't need to sample features at each node, but can use the decision tree constructor you already have.\n",
    "\n",
    "**(8, 3 pts)** Add a few lines to the original decision tree method where, if the RANDOM_FOREST variable is true, all_decisions uses a random sample of its decisions of length sqrt(len(all_decisions)).  (You'll find the function random.sample() handy here.)\n",
    "\n",
    "**(9, 6 pts)** Code a classify() method for your RandomForest that asks its decision trees to vote on a classification, and returns their majority decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "dhU6qNp2Hc0J"
   },
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "\n",
    "  def __init__(self, examples, labels, tree_count):\n",
    "        self.trees = []\n",
    "        for _ in range(tree_count):\n",
    "            # Create a bootstrapped sample for each tree\n",
    "            sample_examples, sample_labels = bag(examples, labels)\n",
    "            # Create a tree from each sample\n",
    "            tree = DecisionTree(sample_examples, sample_labels)\n",
    "            self.trees.append(tree)\n",
    "  \n",
    "  def classify(self, example):\n",
    "        # Collect votes from all trees\n",
    "        votes = [tree.classify(example) for tree in self.trees]\n",
    "        # Majority voting\n",
    "        majority_vote = max(set(votes), key=votes.count)\n",
    "        return majority_vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBQgBLkVHdbM"
   },
   "source": [
    "**(10, 4 pts)** Get the new train and test accuracies for your RandomForest, using either your original evaluate function or a similar one.  The number of trees to create is up to you, but you should at least get similar performance to the single tree with pruning.  You can turn off pruning to speed things up slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "WVdtJQGZHhzh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Training Data Accuracy: 93.06%\n",
      "RandomForest Test Data Accuracy: 83.20%\n"
     ]
    }
   ],
   "source": [
    "ENABLE_PRUNING = False\n",
    "RANDOM_FOREST = True\n",
    "# TODO\n",
    "num_trees = 10\n",
    "my_forest = RandomForest(features_train_list, labels_train_list, num_trees)\n",
    "\n",
    "# Now, let's use the evaluate function to get accuracies\n",
    "train_accuracy = evaluate(my_forest, features_train_list, labels_train_list)\n",
    "test_accuracy = evaluate(my_forest, features_test_list, labels_test_list)\n",
    "\n",
    "print(f\"RandomForest Training Data Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"RandomForest Test Data Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(11, 6 pts)**  One last \"thought question.\"  Suppose we want to train a decision tree to just say \"yes\" to one datapoint and \"no\" to all other points in the data.  Assume all features are continuous.  How many decision nodes (not leaf nodes) are necessary, as a function of the number of continuous features $n$, to make this decision tree say \"yes\" to a high-dimensional cube around the target point, and \"no\" to all points outside the hypercube?  And what is the rough shape of the tree that does this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "\n",
    "To create a decision tree that says \"yes\" only to one specific data point within a high-dimensional space of $n$ continuous features, and \"no\" to all others, we need $2n$ decision nodes. This is because for each continuous feature, we require two decision nodes: one to set the lower boundary and another for the upper boundary of the feature value, effectively isolating the target point within a hypercube.\n",
    "\n",
    "Rough Shape of the Tree:\n",
    "The tree will have a highly unbalanced or linear shape, with a sequence of decision nodes for each feature that sequentially checks the lower and upper bounds. This forms a path leading to a \"yes\" for the target point, while all other paths lead to \"no\".\n",
    "\n",
    "In summary:\n",
    "Number of Decision Nodes Required: $2n$ (for $n$ features).\n",
    "Tree Shape: A linear sequence of decisions, highly unbalanced, leading to a hypercube around the target point.\n",
    "This approach highlights an extreme case of overfitting, creating a model that is overly complex and specific to the training data, with poor generalizability to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gh6_tvKkcIZG"
   },
   "source": [
    "****When you're done, use \"File->Download .ipynb\" and upload your .ipynb file to Blackboard, along with a PDF version (File->Print->Save as PDF) of your assignment.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
