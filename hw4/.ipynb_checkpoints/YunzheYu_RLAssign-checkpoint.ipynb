{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4yvI-kf-myN"
   },
   "source": [
    "# MDPs and Q-learning On \"Ice\" (60 points possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iEuDHFs-ixs"
   },
   "source": [
    "In this assignment, we’ll revisit Markov Decision Processes while also trying out Q-Learning, the reinforcement learning approach that associates utilities with attempting actions in states.\n",
    "The problem that we’re attempting to solve is the following:\n",
    "\n",
    "1.  There is a grid of spaces in a rectangle.  Each space can contain a pit (negative reward), gold (positive reward), or nothing.\n",
    "2.  The rectangle is effectively surrounded by walls, so anything that would move you outside the rectangle, instead moves you to the edge of the rectangle.\n",
    "3.  The floor is icy.  Any attempt to move in a cardinal direction results in moving a somewhat random number of spaces in that direction.  The exact probabilities of moving each number of spaces are given in the problem description.  (If you slide too far, see rule #2.)\n",
    "4.  Landing on a pit or gold effectively “ends the run,” for both a Q learner and an agent later trying out the policy.  It’s game over.  (To simulate this during Q learning, set all Q values for the space to equal its reward, then start over from a random space.)  Note that it's still possible to slide past a pit or gold - this doesn't end the run.\n",
    "\n",
    "A sample input looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "L_PSiQiO-_2y"
   },
   "outputs": [],
   "source": [
    "sampleMDP = \"\"\"0.7 0.2 0.1\n",
    "- - P - -\n",
    "- - G P -\n",
    "- - P - -\n",
    "- - - - -\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CmLASMB_Gsd"
   },
   "source": [
    "\n",
    "The first line says that the probabilities of moving one, two, or three spaces in the direction of movement are 0.7, 0.2, and 0.1.   The rest is a map of the environment, where a dash is an empty space, P is a pit, and G is gold.\n",
    "\n",
    "Your job is to finish the code below for mdp_solve() and q_solve().  These take a problem description like the one pictured above, and return a policy giving the recommended action to take in each empty square (U=up, R=right, D=down, L=left).\n",
    "\n",
    "**1, 17 points)**  mdp_solve() should use value iteration and the Bellman equation.  ITERATIONS will refer to the number of complete passes you perform over all states.  You can initialize the utilities to the rewards of each state.  Don’t update the rewards spaces from their initial rewards; since they end the trial, they have no future utility.  Don't update utilities in-place as you iterate through them, but create a fresh array of utilities with each pass, in order to avoid biasing moves in the directions that have already been updated.\n",
    "\n",
    "**2, 24 points)**  q_solve() will run ITERATIONS trials in which a learner starts in a random empty square and moves until it hits a pit or gold, in which case, the trial is over.  (If it was randomly dropped into gold or a pit, the trial is immediately over.)  The learner moves by deciding randomly whether to choose a random direction (with probability EXPLORE_PROB) or move according to the best Q-value of its current square (otherwise).  Simulate the results of the move on slippery ice to determine where the learner ended up - then apply the Q-learning equation given in lecture and the textbook.  (There are multiple Q-learning variants out there, so try to use the equations and practices described in lecture instead of using other sources, to avoid confusion.)\n",
    "\n",
    "The fact that a trial ends immediately on finding gold or a pit means that we want to handle those spaces in a special way.  Normally Q values are updated on moving to the next state, but we won’t see any next state in these cases.  So, to handle this, when the agent discovers one of these rewards, set all the Q values for that space to the associated reward before quitting the trial.  So, for example, if gold is worth 100 and it’s discovered in square x, Q(x,UP) = 100, Q(x,RIGHT) = 100, Q(x, DOWN) = 100, and Q(x, LEFT) = 100.  There’s no need to apply the rest of the Q update equation when the trial is ending, because that’s all about future rewards, and there’s no future when the trial is ending.  But now the spaces that can reach that space will evaluate themselves appropriately.  (Before being \"discovered,\" the square should have no utility.)\n",
    "\n",
    "You should use the GOLD_REWARD, PIT_REWARD, LEARNING_RATE, and DISCOUNT_FACTOR constants at the top of the code box below.\n",
    "\n",
    "Q-learning involves a lot of randomness and some arbitrary decisions when breaking ties, so two implementations can both be correct but recommend slightly different policies in the end, even if they have the same starting random seed.  While we provide some helpful premade maps below, your main guide for debugging will be common sense in deciding whether the policy created by your agent makes sense -- ie, agents following the policy will get gold without taking unnecessary risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZW7aHFXpUQ9l"
   },
   "outputs": [],
   "source": [
    "\"\"\" \"MDPs on Ice - Assignment 5\"\"\"\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "GOLD_REWARD = 250.0\n",
    "PIT_REWARD = -150.0\n",
    "DISCOUNT_FACTOR = 0.8\n",
    "EXPLORE_PROB = 0.2 # for Q-learning\n",
    "LEARNING_RATE = 0.01\n",
    "ITERATIONS = 20000\n",
    "MAX_MOVES = 1000\n",
    "ACTIONS = 4\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "MOVES = ['U', 'R', 'D', 'L']\n",
    "\n",
    "# Fixed random number generator seed for result reproducibility --\n",
    "# don't use a random number generator besides this to match sol\n",
    "random.seed(340)\n",
    "\n",
    "class Problem:\n",
    "    \"\"\"Represents the physical space, transition probabilities, reward locations, and approach\n",
    "\n",
    "    ...in short, the info in the problem string\n",
    "\n",
    "    Attributes:\n",
    "        move_probs (List[float]):  probabilities of going 1,2,3 spaces\n",
    "        map (List[List(string)]]:  \"-\" (safe, empty space), \"G\" (gold), \"P\" (pit)\n",
    "\n",
    "    String format consumed looks like\n",
    "    0.7 0.2 0.1   [probability of going 1, 2, 3 spaces]\n",
    "    - - - - - - P - - - -   [space-delimited map rows]\n",
    "    - - G - - - - - P - -   [G is gold, P is pit]\n",
    "\n",
    "    You can assume the maps are rectangular, although this isn't enforced\n",
    "    by this constructor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, probstring):\n",
    "        \"\"\" Consume string formatted as above\"\"\"\n",
    "        self.map = []\n",
    "        for i, line in enumerate(probstring.splitlines()):\n",
    "            if i == 0:\n",
    "                self.move_probs = [float(s) for s in line.split()]\n",
    "            else:\n",
    "                self.map.append(line.split())\n",
    "\n",
    "    def solve(self, iterations, use_q):\n",
    "        \"\"\" Wrapper for MDP and Q solvers.\n",
    "\n",
    "        Args:\n",
    "            iterations (int):  Number of iterations (but these work differently for the two solvers)\n",
    "            use_q (bool):  False means use MDP value iteration, true means use Q-learning\n",
    "        Returns:\n",
    "            A Policy, in either case (what to do in each square; see class below)\n",
    "        \"\"\"\n",
    "\n",
    "        if use_q:\n",
    "            return q_solve(self, iterations)\n",
    "        return mdp_solve(self, iterations)\n",
    "\n",
    "class Policy:\n",
    "    \"\"\" Abstraction on the best action to perform in each state.\n",
    "\n",
    "    This is a string list-of-lists map similar to the problem input, but a character gives the best\n",
    "    action to take in each non-reward square (see MOVES constant at top of file).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, problem):\n",
    "        \"\"\"Args:\n",
    "\n",
    "        problem (Problem):  The MDP problem this is a policy for\n",
    "        \"\"\"\n",
    "        self.best_actions = copy.deepcopy(problem.map)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Join the characters in the policy into one big space-separated, multline string\"\"\"\n",
    "        return '\\n{}\\n'.format('\\n'.join([' '.join(row) for row in self.best_actions]))\n",
    "\n",
    "def roll_steps(move_probs, row, col, move, rows, cols):\n",
    "    \"\"\"Calculates the new coordinates that result from a move.\n",
    "\n",
    "    Includes the \"roll of the dice\" for transition probabilities and checking arena boundaries.\n",
    "\n",
    "    Helper for try_policy and q_solve - probably useful in your Q-learning implementation.\n",
    "\n",
    "    Args:\n",
    "        move_probs (List[float]):  Transition probabilities for the ice (from problem)\n",
    "        row, col (int, int):  location of agent before moving\n",
    "        move (string):  The direction of move as a MOVES character (not an int constant!)\n",
    "        rows, cols (int, int):  number of rows and columns in the map\n",
    "\n",
    "    Returns:\n",
    "        new_row, new_col (int, int):  The new row and column after moving\n",
    "    \"\"\"\n",
    "    displacement = 1\n",
    "    total_prob = 0\n",
    "    move_sample = random.random()\n",
    "    for p, prob in enumerate(move_probs):\n",
    "        total_prob += prob\n",
    "        if move_sample <= total_prob:\n",
    "            displacement = p+1\n",
    "            break\n",
    "    # Handle \"slipping\" into edge of map\n",
    "    new_row = row\n",
    "    new_col = col\n",
    "    if not isinstance(move, str):\n",
    "        print(\"Warning: roll_steps wants str for move, got a different type\")\n",
    "    if move == \"U\":\n",
    "        new_row -= displacement\n",
    "        if new_row < 0:\n",
    "            new_row = 0\n",
    "    elif move == \"R\":\n",
    "        new_col += displacement\n",
    "        if new_col >= cols:\n",
    "            new_col = cols-1\n",
    "    elif move == \"D\":\n",
    "        new_row += displacement\n",
    "        if new_row >= rows:\n",
    "            new_row = rows-1\n",
    "    elif move == \"L\":\n",
    "        new_col -= displacement\n",
    "        if new_col < 0:\n",
    "            new_col = 0\n",
    "    return new_row, new_col\n",
    "\n",
    "\n",
    "def try_policy(policy, problem, iterations):\n",
    "    \"\"\"Returns average utility per move of the policy.\n",
    "\n",
    "    Average utility is as measured by \"iterations\" random drops of an agent onto empty\n",
    "    spaces, running until gold, pit, or time limit MAX_MOVES is reached.\n",
    "\n",
    "    Doesn't necessarily play a role in your code, but you can try policies this\n",
    "    way\n",
    "\n",
    "    Args:\n",
    "        policy (Policy):  the policy the agent is following\n",
    "        problem (Problem):  the environment description\n",
    "        iterations (int):  the number of random trials to run\n",
    "    \"\"\"\n",
    "    total_utility = 0\n",
    "    total_moves = 0\n",
    "    for _ in range(iterations):\n",
    "        # Resample until we have an empty starting square\n",
    "        while True:\n",
    "            row = random.randrange(0, len(problem.map))\n",
    "            col = random.randrange(0, len(problem.map[0]))\n",
    "            if problem.map[row][col] == \"-\":\n",
    "                break\n",
    "        for moves in range(MAX_MOVES):\n",
    "            total_moves += 1\n",
    "            policy_rec = policy.best_actions[row][col]\n",
    "            # Take the move - roll to see how far we go, bump into map edges as necessary\n",
    "            row, col = roll_steps(problem.move_probs, row, col, policy_rec, \\\n",
    "                                  len(problem.map), len(problem.map[0]))\n",
    "            if problem.map[row][col] == \"G\":\n",
    "                total_utility += GOLD_REWARD\n",
    "                break\n",
    "            if problem.map[row][col] == \"P\":\n",
    "                total_utility += PIT_REWARD\n",
    "                break\n",
    "    return total_utility / total_moves\n",
    "\n",
    "def mdp_solve(problem, iterations):\n",
    "    \"\"\" Perform value iteration for the given number of iterations on the MDP problem.\n",
    "\n",
    "    Here, the squares with rewards can be initialized to the reward values, since value iteration\n",
    "    assumes complete knowledge of the environment and its rewards.\n",
    "\n",
    "    Args:\n",
    "        problem (Problem):  description of the environment\n",
    "        iterations (int):  number of complete passes over the utilities\n",
    "    Returns:\n",
    "        a Policy (though you may design this to return utilities as a second return value)\n",
    "    \"\"\"\n",
    "    # TODO calculate the policy\n",
    "    rows = len(problem.map)\n",
    "    cols = len(problem.map[0])\n",
    "    utilities = [[0 for _ in range(cols)] for _ in range(rows)]\n",
    "    # Initialize utilities based on problem map\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if problem.map[r][c] == 'G':\n",
    "                utilities[r][c] = GOLD_REWARD\n",
    "            elif problem.map[r][c] == 'P':\n",
    "                utilities[r][c] = PIT_REWARD\n",
    "    \n",
    "    # Value iteration\n",
    "    for _ in range(iterations):\n",
    "        new_utilities = copy.deepcopy(utilities)\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                if problem.map[r][c] in ['G', 'P']:\n",
    "                    continue  # Skip gold and pit cells, their utility remains constant\n",
    "                action_utilities = []\n",
    "                for move, action in enumerate(MOVES):  # Check utility for each action\n",
    "                    total_expected_utility = 0\n",
    "                    for displacement, prob in enumerate(problem.move_probs):\n",
    "                        new_r, new_c = roll_steps([prob], r, c, action, rows, cols)\n",
    "                        total_expected_utility += prob * utilities[new_r][new_c]\n",
    "                    action_utilities.append(total_expected_utility)\n",
    "                new_utilities[r][c] = max(action_utilities)  # Bellman update\n",
    "\n",
    "        utilities = new_utilities  # Update utilities after all states considered\n",
    "\n",
    "    # Generate policy from utilities\n",
    "    policy = Policy(problem)\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if problem.map[r][c] in ['-']:\n",
    "                best_action = None\n",
    "                best_utility = float('-inf')\n",
    "                for move, action in enumerate(MOVES):\n",
    "                    expected_utility = 0\n",
    "                    for displacement, prob in enumerate(problem.move_probs):\n",
    "                        new_r, new_c = roll_steps([prob], r, c, action, rows, cols)\n",
    "                        expected_utility += prob * utilities[new_r][new_c]\n",
    "                    if expected_utility > best_utility:\n",
    "                        best_action = action\n",
    "                        best_utility = expected_utility\n",
    "                policy.best_actions[r][c] = best_action\n",
    "    return policy\n",
    "\n",
    "def q_solve(problem, iterations):\n",
    "    \"\"\"q_solve:  Use Q-learning to find a good policy on an MDP problem.\n",
    "\n",
    "    Each iteration corresponds to a random drop of the agent onto the map, followed by moving\n",
    "    the agent until a reward is reached or MAX_MOVES moves have been made.  When an agent\n",
    "    is sitting on a reward, update the utility of each move from the space to the reward value\n",
    "    and end the iteration.  (For simplicity, the agent also does this if just dropped there.)\n",
    "    The agent does not \"know\" reward locations in its Q-values before encountering the space\n",
    "    and \"discovering\" the reward.\n",
    "\n",
    "    Note that texts differ on when to pay attention to this reward - this code follows the\n",
    "    convention of scoring rewards of the space you are moving *from*, plus discounted best q-value\n",
    "    of where you landed.\n",
    "\n",
    "    Assume epsilon-greedy exploration.  Leave reward letters as-is in the policy,\n",
    "    to make it more readable.\n",
    "\n",
    "    Args:\n",
    "        problem (Problem):  The environment\n",
    "        iterations (int):  The number of runs from random start to reward encounter\n",
    "    Returns:\n",
    "        A Policy for the map\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    rows, cols = len(problem.map), len(problem.map[0])\n",
    "    # Initialize Q-table: rows x cols x ACTIONS (4 for up, right, down, left)\n",
    "    Q = [[[0.0 for _ in range(ACTIONS)] for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # Random starting point\n",
    "        r, c = random.choice([(r, c) for r in range(rows) for c in range(cols) if problem.map[r][c] == '-'])\n",
    "        for move_count in range(MAX_MOVES):\n",
    "            # Decide whether to explore or exploit\n",
    "            if random.random() < EXPLORE_PROB:\n",
    "                action = random.randint(0, ACTIONS - 1)  # Explore: random action\n",
    "            else:\n",
    "                action = Q[r][c].index(max(Q[r][c]))  # Exploit: choose best action from Q-table\n",
    "\n",
    "            # Move and calculate new state\n",
    "            new_r, new_c = roll_steps(problem.move_probs, r, c, MOVES[action], rows, cols)\n",
    "\n",
    "            # Determine the reward\n",
    "            if problem.map[new_r][new_c] == 'G':\n",
    "                reward = GOLD_REWARD\n",
    "            elif problem.map[new_r][new_c] == 'P':\n",
    "                reward = PIT_REWARD\n",
    "            else:\n",
    "                reward = 0  # Default reward for other moves\n",
    "\n",
    "            # Update Q-value for the action taken\n",
    "            if problem.map[new_r][new_c] in ['G', 'P']:  # Terminal state\n",
    "                # Set all Q-values for the terminal state to its corresponding reward\n",
    "                Q[new_r][new_c] = [reward] * ACTIONS\n",
    "                # End this trial since the agent has reached a terminal state\n",
    "                break\n",
    "            else:\n",
    "                # Standard Q-learning update rule\n",
    "                best_future_q = max(Q[new_r][new_c])\n",
    "                Q[r][c][action] += LEARNING_RATE * (reward + DISCOUNT_FACTOR * best_future_q - Q[r][c][action])\n",
    "\n",
    "            # Update state to new state\n",
    "            r, c = new_r, new_c\n",
    "\n",
    "    # Generate policy from Q-table\n",
    "    policy = Policy(problem)\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if problem.map[r][c] == '-':\n",
    "                best_action_index = Q[r][c].index(max(Q[r][c]))\n",
    "                policy.best_actions[r][c] = MOVES[best_action_index]\n",
    "            else:\n",
    "                policy.best_actions[r][c] = problem.map[r][c]  # Leave rewards and pits as they are\n",
    "\n",
    "    return policy\n",
    "\n",
    "def new_q(rewards, utilities, r, c, new_r, new_c, movenum):\n",
    "    \"\"\" Q-learning function.  Returns the new Q-value for space (r,c).\n",
    "    It's recommended you code and test this before doing the overall Q-learning.\n",
    "\n",
    "    Should use the LEARNING_RATE and DISCOUNT_FACTOR.\n",
    "\n",
    "    Args:\n",
    "        rewards (List[List[float]]):  Reward amounts built into the problem map (indexed [r][c])\n",
    "        utilities (List[List[List[float]]]):  The Q-values for each action from each space.\n",
    "                                              (Indexed as [row][col][move])\n",
    "        r, c (int, int):  Row and column of our location before move\n",
    "        new_r, new_c (int, int):  Row and column of our location after move\n",
    "        movenum (int):  Integer index into the Q-values, corresponding to constants UP etc\n",
    "    Returns:\n",
    "        float - the new Q-value for the space we moved from\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "        # Extract the current Q value for the state-action pair\n",
    "    current_q_value = utilities[r][c][movenum]\n",
    "    # Calculate the reward for moving to the new state\n",
    "    reward = rewards[new_r][new_c]\n",
    "    # Find the maximum Q-value among all possible actions from the new state\n",
    "    max_future_q = max(utilities[new_r][new_c])\n",
    "    # Compute the updated Q-value using the Q-learning formula\n",
    "    updated_q_value = current_q_value + LEARNING_RATE * (reward + DISCOUNT_FACTOR * max_future_q - current_q_value)\n",
    "    \n",
    "\n",
    "    return updated_q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5cuaEcLvAoYK"
   },
   "outputs": [],
   "source": [
    "deterministic_test = \"\"\"1.0\n",
    "- - P - -\n",
    "- - G P -\n",
    "- - P - -\n",
    "- - - - -\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3ABkxRiTA4Wi"
   },
   "outputs": [],
   "source": [
    "# Notice that we counterintuitively are most likely to go 2 spaces here\n",
    "very_slippy_test = \"\"\"0.2 0.7 0.1\n",
    "- - P - -\n",
    "- - G P -\n",
    "- - P - -\n",
    "- - - - -\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Lqg8ZZUCBYYl"
   },
   "outputs": [],
   "source": [
    "big_test = \"\"\"0.6 0.3 0.1\n",
    "- P - G - P - - G -\n",
    "P G - P - - - P - -\n",
    "P P - P P - P - P -\n",
    "P - - P P - - - - P\n",
    "- - - - - - - - P G\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sHZ99I9uBmiH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U U P U U\n",
      "U U G P U\n",
      "U U P R U\n",
      "U U R U U\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MDP value iteration tests\n",
    "print(Problem(deterministic_test).solve(ITERATIONS, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "txLGS4pUwhh7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U U P U U\n",
      "U U G P U\n",
      "U U P R U\n",
      "U U R U U\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Problem(sampleMDP).solve(ITERATIONS, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WnprAX2uwiDI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U U P U U\n",
      "U U G P U\n",
      "U U P R U\n",
      "U U R U U\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Problem(very_slippy_test).solve(ITERATIONS, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "INhKxA6twic8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U P U G U P U U G U\n",
      "P G U P U R U P U U\n",
      "P P U P P U P D P U\n",
      "P R U P P U R U L P\n",
      "R U U R R U U U P G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Problem(big_test).solve(ITERATIONS, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LfUJKMPtCRCs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U U P U U\n",
      "U U G P U\n",
      "U U P U U\n",
      "U U U U U\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q-learning tests\n",
    "# Set seed every time for consistent executions;\n",
    "# comment out to get different random runs\n",
    "random.seed(340)\n",
    "print(Problem(deterministic_test).solve(ITERATIONS, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "08cHCoI6wqak"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U U P U U\n",
      "U U G P U\n",
      "U U P U U\n",
      "U U U U U\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(340)\n",
    "print(Problem(sampleMDP).solve(ITERATIONS, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PMM3kelxwqsx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U U P U U\n",
      "U U G P U\n",
      "U U P U U\n",
      "U U U U U\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(340)\n",
    "print(Problem(very_slippy_test).solve(ITERATIONS, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "hWu_w30AwrP9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U P U G U P U U G U\n",
      "P G U P U U U P U U\n",
      "P P U P P U P U P U\n",
      "P U U P P U U U U P\n",
      "U U U U U U U U P G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(340)\n",
    "print(Problem(big_test).solve(ITERATIONS, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbP5VrvF9dIt"
   },
   "source": [
    "Once you're done, here are a few thought questions (19 points total):\n",
    "\n",
    "**3, 5 points) Suppose we are on the deterministic map where there is no sliding on ice, and performing value iteration until it converges.  Supposing 0 < DISCOUNT_FACTOR < 1, how does the policy change if the discount factor changes to another value in that range (or does the policy change at all)?  Why does that happen?  What happens to the policy if DISCOUNT_FACTOR = 1?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzD5X8wY-5gS"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "When performing value iteration on a deterministic map:\n",
    "\n",
    "If 0 < DISCOUNT_FACTOR < 1: Changing the discount factor alters the balance between valuing immediate versus future rewards. A lower discount factor makes the policy favor immediate rewards more, becoming short-sighted. A higher discount factor makes the policy value future rewards more, becoming far-sighted. Therefore, the policy can change depending on whether immediate or future rewards are prioritized.\n",
    "\n",
    "If DISCOUNT_FACTOR = 1: The policy treats all rewards, immediate and future, equally. This can lead to a focus on maximizing long-term rewards without regard for immediacy, which might not always be practical. However, this setting can cause convergence issues and might not accurately reflect realistic decision-making scenarios where future uncertainties should diminish the value of distant rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CkLwZnX-87e"
   },
   "source": [
    "**4, 3 points) The value iteration MDP solver updates all squares an equal number of times.  The Q-learner does not.  Which squares might we expect the Q-learner to update the most?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEhHogqO_7fC"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "The Q-learner is likely to update squares that are frequently visited more often. These often include starting points, common paths to rewards, and areas near the initial positions due to random exploration. Squares near rewards or common routes may also see more updates due to their higher attractiveness for learning optimal paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeGcHwKR_-m7"
   },
   "source": [
    "**5, 11 points) Suppose we change the state information so that, instead of knowing its coordinates on the map, the agent instead knows just the locations of all rewards in a 5x5 square with the agent at the square center.  Thus, at the start of every run, it may not know exactly where it is, but it knows what is in the vicinity.  It also does not know the transition model.**\n",
    "\n",
    "**a, 2 points) We can't use value iteration here.  Why?**\n",
    "\n",
    "**b, 4 points) How many state-action combinations are possible, assuming the contents of the agent's own square don't matter, and every other square could have a pit, gold, or an empty square as in the example maps?  Is a lookup table of Q-values feasible if we allocate memory for each possible state-action combination?  (Let's define \"feasible\" as \"able to be stored in a gig or less of memory,\" assuming 64-bit values.)**\n",
    "\n",
    "**c, 5 points) Let's suppose we want to instead generate Q-values with a classic neural network with a single hidden layer.  The inputs are the contents of the 24 squares in the 5x5 square that the player is not  in (we can encode gold = 1, nothing = 0, pit = -1).  There are 10 hidden units.  There are 4 output units corresponding to the 4 possible actions' Q-values.  How much memory is required for the weights of this network, assuming each is a 32-bit float (don't forget bias weights for each unit)?  Comparing to part (b), is it more efficient in memory to use a lookup table for Q(s,a), or this neural network?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Inb8brIUIk8U"
   },
   "source": [
    "**a)**\n",
    "\n",
    "Value iteration requires knowledge of the exact state (coordinates on the map) and the transition model to update the utilities of each state based on the possible outcomes of actions. If the agent only knows about rewards within a 5x5 vicinity and lacks the exact coordinates or transition model, it can't predict the outcome of its actions accurately for the whole map, making value iteration inapplicable.\n",
    "\n",
    "**b)**\n",
    "\n",
    "There are 3 possible conditions for each of the 24 squares around the agent (pit, gold, empty), so the total number of state configurations is $3^{24}$. With 4 possible actions, there are $3^{24} * 4$ state-action combinations. Assuming each Q-value is a 64-bit value (8 bytes):\n",
    "\n",
    "Total memory = $3^{24} * 4 * 8$ bytes.\n",
    "\n",
    "We need to calculate this to see if it's under one gigabyte (which is $2^{30}$ bytes.)\n",
    "\n",
    "The total number of state-action combinations requires approximately 8417.06 GB of memory, which is not feasible as it exceeds our 1 GB limit.\n",
    "\n",
    "**c)**\n",
    "\n",
    "For the neural network:\n",
    "\n",
    "Input to hidden layer weights: $24 * 10 = 240$ weights.\n",
    "\n",
    "Bias weights for hidden layer: 10.\n",
    "\n",
    "Hidden to output layer weights: $10 * 4 = 40$ weights.\n",
    "\n",
    "Bias weights for output layer: 4.\n",
    "\n",
    "Total weights = 240 + 10 + 40 + 4 = 294.\n",
    "\n",
    "Since each weight is a 32-bit float (4 bytes):\n",
    "\n",
    "Total memory for NN = $294 * 4$ bytes.\n",
    "\n",
    "The memory required for the weights of the neural network is approximately 1.15 KB, which is significantly less than the memory needed for the lookup table of Q-values.\n",
    "\n",
    "Comparing the two, it is far more efficient in terms of memory to use a neural network rather than a lookup table for storing Q-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78PjFoZBOLfp"
   },
   "source": [
    "**Remember to submit your code on Blackboard as both an .ipynb (File->Download->.ipynb) and a PDF (Print->Save as PDF).**"
   ]
  }
 ],
 "metadata": {
  "author": [
   {
    "@type": "Person",
    "name": "Kevin Gold"
   }
  ],
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
